
## CPU算法设计
---

1. 设计Matrix类简化代码
	- 重载运算符使得其使用变得自然
	- 设计map和changemap方法使得矩阵可以方便地进行各种操作

2. 将梯度计算和代价计算合并在一个函数中，可以相互利用计算结果

## GPU算法设计
---

1. 精确使用显存
	- 尽可能减少临时矩阵的存储，将连续的map操作合并到一起
	- 合理安排计算顺序，在一个矩阵使命结束后其空间可以存放另一矩阵
	- 将矩阵数据分储在各个线程的寄存器中，仅在需要做乘法时写入全局内存

2. 虚拟化矩阵设计
	- 算法中出现多次“添加一列”和“删除第一列”操作，而如果改写矩阵的话将会消耗较多的时间，为此我们设计了虚拟化矩阵的方法，即在矩阵结构体描述中增加标志位，在读取第一列的时候查看标志位来决定是读取内存还是直接返回虚拟的值。

3. 核功能函数设计
	- 
	
4. 核过程函数设计
	- 由于需要全局同步，我们将每次循环的过程分成了6个阶段，每个阶段之间需要退回host端保证全局同步。我们设计的6个核过程函数如下。

5. host主函数设计 


## CPU与GPU算法比较
---

1. #####简单函数实现
	- 使用宏代替函数调用

	- 例：
		- 伪代码：
			
			```
			sigmoid(z) = 1.0 / (1.0 + exp(-z));
			
			```
		- CPU实现
		
			```
			template<typename T>
			T sigmoid(T x)
			{
    			return (T)1 / ((T)1 + exp(-x));
			}

			```
		- 优化
			
			```
			#define sigmoid(x) (1.0f/(1.0f+exp(-(x))))
			``` 
			
2. #####算法优化
	- 我们发现如下两个函数之间有如下关系。
	
		```
		sigmoidGradient(z)=sigmoid(z).*(1-sigmoid(z));
		```
		- 我们如果不保留z而是保留```temp=sigmoid(z)```，我们就可以节省一块存储空间并且实现这个函数。```sigmoidGradient(z)=temp*(1-temp)```
		
3. #####矩阵运算优化
	
	- 使用虚拟化矩阵方案，减少了拷贝和新矩阵构建
		- 我们仔细研究了算法中所需要的运算，矩阵中有多次需要将一个矩阵去掉第一列，或将矩阵左侧链接一列1的操作。
		- CPU端，未优化
			- 使用新矩阵来存储加/减行列之后的矩阵
			
			```
			M temp1(1.0f,X.row,1);
    		M a1 = temp1.rightlink(X);
			```
		- GPU端，优化后
			- 使用矩阵结构体中的一个标志标志其虚拟化状态，加一列、减一列还是正常未虚拟化矩阵。
			- 我们使用一个宏来返回该矩阵```M(i,j)```需要返回的值。在这个宏定义中，我们将i,j映射到M指针指到的位置上，同时考虑到虚拟化问题。
			
			```
			#define index(M1,i,j) (i)*(M1.col+(M1).flag)+(j)+(M1).flag
			#define getitem(M1,i,j) (((j)==0 && ((M1).flag)==-1)? 1:(M1).ptr[index(M1,i,j)])
			```
			- 需要左侧加一列，只需要调整标志位和列数即可。
			
			```
			M a1 = X;
			a1.col++;
			a1.flag=-1;
			```
	- 使用三个乘法运算，修改矩阵乘法法则，省去了矩阵转置这个运算。
		- 由于我们的矩阵运算大都是需要转置的，计算```A'*B , A*B , A*B'```三种形式的矩阵运算。我们优化后使用3个函数```_mul , mul , mul_```实现这三种乘法，使得可以省去转置这个操作，直接得到结果。
		- CPU端
			- 转置之后进行运算
			
			```
			M Theta1_grad = ((~ d2 )*a1).map([=](DT x,int,int){return x / SAMPLE_NUM;});
			```
		- GPU端
			
			```
			a2.ptr[gid] = sigmoid(mul_(a1,theta1,gid));
			```
			
## 结果展示
---
